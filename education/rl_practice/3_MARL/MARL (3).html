
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Markov Games for Multi-Agent RL: Littman’s Soccer Experiment &#8212; Selected advanced topics in reinforcement learning: practical hands-on</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../static/doctools.js?v=9a2dae69"></script>
    <script src="../static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../static/copybutton.js?v=f281be69"></script>
    <script src="../static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '3_MARL/MARL (3)';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Selected advanced topics in reinforcement learning: practical hands-on</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../1_Basic_RL/1_RL_basics.html">Reinforcement learning basics with CartPole model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_Inverse_RL/2_IRL.html">Inverse Reinforcement Learning with Grid World Traversal</a></li>
<li class="toctree-l1"><a class="reference internal" href="MARL.html">Markov Games for Multi-Agent RL: Littman’s Soccer Experiment</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../sources/3_MARL/MARL (3).ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Markov Games for Multi-Agent RL: Littman’s Soccer Experiment</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Markov Games for Multi-Agent RL: Littman’s Soccer Experiment</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-definition"><strong>1 Problem Definition</strong>.</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-decision-processes-mdps">1.1 Markov Decision Processes (MDPs)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#two-player-zero-sum-markov-games">1.2 Two-Player Zero-Sum Markov Games</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-optimality-contrast">1.3 Policy Optimality Contrast</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fundamental-differences">Fundamental Differences</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#research-challenges-experimental-goals">1.4 Research Challenges &amp; Experimental Goals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-challenges">Key Challenges</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#experimental-framework">Experimental Framework</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation"><strong>2 Implementation</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#environment-setup-with-code">2.1 Environment Setup with Code</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reward-mechanism-implementation">2.2 Reward Mechanism Implementation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithm-comparison">2.3 Algorithm Comparison</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-configuration">2.4 Training Configuration</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#experiments"><strong>3 Experiments</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#complete-small-scale-experiment">3.1 Complete Small-Scale Experiment</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experimental-results-analysis">3.2. Experimental Results Analysis</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-performance-metrics">1. Key Performance Metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#action-selection-patterns">2. Action Selection Patterns</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exploration-convergence-dynamics">3. Exploration-Convergence Dynamics</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#core-conclusions">4 Core Conclusions</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-optimization-recommendations">4.1. Parameter Optimization Recommendations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithm-parameters">1. Algorithm Parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#environmental-modifications">2. Environmental Modifications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#architectural-improvements">3 Architectural Improvements</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-optimization-roadmap">4.2. Performance Optimization Roadmap</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extended-experiment-proposals">4.3. Extended Experiment Proposals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-opponent-strategy">1. <strong>Dynamic Opponent Strategy</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#spatial-reward-shaping">2. <strong>Spatial Reward Shaping</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning-test">3. <strong>Transfer Learning Test</strong></a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="markov-games-for-multi-agent-rl-littman-s-soccer-experiment">
<h1>Markov Games for Multi-Agent RL: Littman’s Soccer Experiment<a class="headerlink" href="#markov-games-for-multi-agent-rl-littman-s-soccer-experiment" title="Link to this heading">#</a></h1>
<p><strong>Based on “Markov Games as a Framework for Multi-Agent RL” (Littman, 1994)</strong></p>
<p>This section demonstrates the minimax-Q learning algorithm using a simple two-player zero-sum Markov game modeled after the game of soccer.</p>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://courses.cs.duke.edu/spring07/cps296.3/littman94markov.pdf">Markov games as a framework for multi-agent reinforcement learning
Michael L. Littma</a></p></li>
<li><p><a class="reference external" href="https://www.kaggle.com/c/rock-paper-scissors">Rock, Paper, Scissors Kaggle Competition</a></p></li>
</ul>
<hr class="docutils" />
</section>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="problem-definition">
<h1><strong>1 Problem Definition</strong>.<a class="headerlink" href="#problem-definition" title="Link to this heading">#</a></h1>
<section id="markov-decision-processes-mdps">
<h2>1.1 Markov Decision Processes (MDPs)<a class="headerlink" href="#markov-decision-processes-mdps" title="Link to this heading">#</a></h2>
<p>Markov Decision Processes (MDPs)[Howard, 1960] provide a mathematical framework for modeling sequential decision-making under uncertainty. Formally defined by the tuple <span class="math notranslate nohighlight">\((S, A, T, R, \gamma)\)</span>, an MDP consists of:</p>
<ul class="simple">
<li><p><strong>State space</strong> <span class="math notranslate nohighlight">\(S\)</span> representing environment configurations</p></li>
<li><p><strong>Action space</strong> <span class="math notranslate nohighlight">\(A\)</span> defining possible decisions</p></li>
<li><p><strong>Transition function</strong> <span class="math notranslate nohighlight">\(T(s'|s,a) \in \text{PD}(S)\)</span> specifying state dynamics</p></li>
<li><p><strong>Reward function</strong> <span class="math notranslate nohighlight">\(R(s,a) \in \mathbb{R}\)</span> quantifying immediate outcomes</p></li>
<li><p><strong>Discount factor</strong> <span class="math notranslate nohighlight">\(\gamma \in [0,1)\)</span> controlling temporal preference</p></li>
</ul>
<p>The agent seeks a policy <span class="math notranslate nohighlight">\(\pi: S \rightarrow A\)</span> maximizing the <em>expected discounted return</em>:
$<span class="math notranslate nohighlight">\(
V^\pi(s) = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t R(s_t,a_t)\right] \tag{1}
\)</span><span class="math notranslate nohighlight">\(
where \)</span>\gamma=0<span class="math notranslate nohighlight">\( reduces to myopic optimization, while \)</span>\gamma \rightarrow 1$ emphasizes long-term outcomes.</p>
<img src="figs/MDP.png" width="100%" />
<p>Figure 1 shows the MDP decision flow: state → action → next state cycle</p>
</section>
<section id="two-player-zero-sum-markov-games">
<h2>1.2 Two-Player Zero-Sum Markov Games<a class="headerlink" href="#two-player-zero-sum-markov-games" title="Link to this heading">#</a></h2>
<p>Extending MDPs to competitive multi-agent scenarios, a zero-sum Markov game [Owen, 1982] is defined by:</p>
<ul class="simple">
<li><p><strong>Joint state space</strong> <span class="math notranslate nohighlight">\(S\)</span></p></li>
<li><p><strong>Dual action spaces</strong> <span class="math notranslate nohighlight">\(A\)</span> (agent) and <span class="math notranslate nohighlight">\(O\)</span> (opponent)</p></li>
<li><p><strong>Competitive transition</strong> <span class="math notranslate nohighlight">\(T(s'|s,a,o) \in \text{PD}(S)\)</span></p></li>
<li><p><strong>Antagonistic reward</strong> <span class="math notranslate nohighlight">\(R(s,a,o) \in \mathbb{R}\)</span> with <span class="math notranslate nohighlight">\(\min_o R = -\max_a R\)</span></p></li>
</ul>
<p>The minimax objective becomes:
$<span class="math notranslate nohighlight">\(
V^*(s) = \max_\pi \min_\sigma \mathbb{E}\left[\sum_{t=0}^\infty \gamma^t R(s_t,a_t,o_t)\right] \tag{2}
\)</span><span class="math notranslate nohighlight">\(
where \)</span>\pi: S \rightarrow \text{PD}(A)<span class="math notranslate nohighlight">\( and \)</span>\sigma: S \rightarrow \text{PD}(O)$ denote mixed strategies.</p>
<p>Consider the Rock-Paper-Scissors game: deterministic strategies lead to exploitation (e.g., always choosing Rock loses to Paper), whereas probabilistic Nash equilibria require uniform randomization.</p>
</section>
<section id="policy-optimality-contrast">
<h2>1.3 Policy Optimality Contrast<a class="headerlink" href="#policy-optimality-contrast" title="Link to this heading">#</a></h2>
<section id="fundamental-differences">
<h3>Fundamental Differences<a class="headerlink" href="#fundamental-differences" title="Link to this heading">#</a></h3>
<p>While MDPs permit deterministic optimal policies
(<span class="math notranslate nohighlight">\(\exists \pi^*: S \rightarrow A\)</span>), Markov games necessitate probabilistic strategies due to adversarial inference:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Criterion</strong></p></th>
<th class="head"><p>MDP</p></th>
<th class="head"><p>Markov Game</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Optimality Basis</p></td>
<td><p>Bellman Optimality</p></td>
<td><p>Minimax Equilibrium</p></td>
</tr>
<tr class="row-odd"><td><p>Strategy Determinism</p></td>
<td><p>Always achievable</p></td>
<td><p>Generally impossible</p></td>
</tr>
<tr class="row-even"><td><p>Opponent Adaptation</p></td>
<td><p>Not required</p></td>
<td><p>Critical survival</p></td>
</tr>
</tbody>
</table>
</div>
<img src="figs/MDPvsGame.png" width="100%" />
<p>Figure 2: MDP single path decision vs Markov Game</p>
</section>
</section>
<section id="research-challenges-experimental-goals">
<h2>1.4 Research Challenges &amp; Experimental Goals<a class="headerlink" href="#research-challenges-experimental-goals" title="Link to this heading">#</a></h2>
<section id="key-challenges">
<h3>Key Challenges<a class="headerlink" href="#key-challenges" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Non-stationarity</strong>: Opponent’s adaptive learning breaks MDP’s environmental stationarity</p></li>
<li><p><strong>Equilibrium Complexity</strong>: Curse of dimensionality in joint strategy space</p></li>
<li><p><strong>Credit Assignment</strong>: Disentangling self/opponent contribution to outcomes</p></li>
</ol>
</section>
<section id="experimental-framework">
<h3>Experimental Framework<a class="headerlink" href="#experimental-framework" title="Link to this heading">#</a></h3>
<p>We design experiments to investigate:</p>
<ol class="arabic simple">
<li><p><strong>Convergence Analysis</strong>: Q-learning variants under minimax objectives (Theorem 1)</p></li>
<li><p><strong>Discount Sensitivity</strong>: Phase transitions in <span class="math notranslate nohighlight">\(\gamma\)</span>-dependent strategies</p></li>
<li><p><strong>Stochasticity Necessity</strong>: Empirical validation of mixed-strategy dominance</p></li>
<li><p><strong>Scalability Limits</strong>: State-space complexity vs learning stability</p></li>
</ol>
<img src="figs/football.png" width="100%" />
<p>Figure 3 : An initial board (left) and a situation requiring a probabilistic choice for A (right)</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="implementation">
<h1><strong>2 Implementation</strong><a class="headerlink" href="#implementation" title="Link to this heading">#</a></h1>
<p>Here is a general code implementation of “definition and theory”</p>
<section id="environment-setup-with-code">
<h2>2.1 Environment Setup with Code<a class="headerlink" href="#environment-setup-with-code" title="Link to this heading">#</a></h2>
<p><strong>Soccer Simulation Framework</strong><br />
We implement a 4×5 grid environment with asymmetric goal placements for adversarial gameplay. Key configurations include:</p>
<ul class="simple">
<li><p><strong>Initial Positions</strong>: Team A (2,1) vs Team B (2,3)</p></li>
<li><p><strong>Dynamic Ball Possession</strong>: Randomized initial ball control</p></li>
<li><p><strong>Action Space</strong>: 5 basic movements (N/S/E/W/Stay) with collision resolution</p></li>
</ul>
<p><strong>Operational Outcomes</strong><br />
After initialization:</p>
<ol class="arabic simple">
<li><p>Generates a unique starting state with visualized player positions</p></li>
<li><p>Displays ball possession through starred markers (A*/B*)</p></li>
<li><p>Highlights goal zones with colored boundaries (red/blue)</p></li>
<li><p>Enables observation of position updates through successive interactions</p></li>
</ol>
<p>The visualization provides immediate spatial understanding of agent positioning, ball dynamics, and goal locations - critical for observing subsequent learning behaviors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">clear_output</span>

<span class="k">class</span> <span class="nc">SoccerEnv</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grid_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">goals</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;A&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="s1">&#39;B&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actions</span> <span class="o">=</span> <span class="p">{</span>
            <span class="mi">0</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>  <span class="c1"># N</span>
            <span class="mi">1</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>   <span class="c1"># S</span>
            <span class="mi">2</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>   <span class="c1"># E</span>
            <span class="mi">3</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># W</span>
            <span class="mi">4</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>    <span class="c1"># Stay</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>


    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize positions and random ball possession&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">A_pos</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">B_pos</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ball_holder</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">])</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_state</span><span class="p">()</span>


    <span class="k">def</span> <span class="nf">_get_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Encode state as tuple for hashing&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">A_pos</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">B_pos</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ball_holder</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action_A</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">action_B</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Parameters should be action indices (0-4)&quot;&quot;&quot;</span>
        <span class="c1"># Get movement directions from action indices</span>
        <span class="n">delta_A</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">actions</span><span class="p">[</span><span class="n">action_A</span><span class="p">]</span>
        <span class="n">delta_B</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">actions</span><span class="p">[</span><span class="n">action_B</span><span class="p">]</span>

        <span class="c1"># Calculate new positions (move first then handle collisions)</span>
        <span class="n">new_A</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_move</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A_pos</span><span class="p">,</span> <span class="n">delta_A</span><span class="p">)</span>
        <span class="n">new_B</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_move</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">B_pos</span><span class="p">,</span> <span class="n">delta_B</span><span class="p">)</span>

        <span class="c1"># Handle collisions (cannot occupy same cell)</span>
        <span class="k">if</span> <span class="n">new_A</span> <span class="o">==</span> <span class="n">new_B</span><span class="p">:</span>
            <span class="c1"># Randomly decide who moves successfully</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">:</span>
                <span class="n">new_A</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">A_pos</span>  <span class="c1"># A stays</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_B</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">B_pos</span>  <span class="c1"># B stays</span>

        <span class="c1"># Update positions with boundary checks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">A_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_clip_position</span><span class="p">(</span><span class="n">new_A</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">B_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_clip_position</span><span class="p">(</span><span class="n">new_B</span><span class="p">)</span>

        <span class="c1"># Check ball possession transfer</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">A_pos</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">B_pos</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ball_holder</span> <span class="o">=</span> <span class="s1">&#39;B&#39;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ball_holder</span> <span class="o">==</span> <span class="s1">&#39;A&#39;</span> <span class="k">else</span> <span class="s1">&#39;A&#39;</span>

        <span class="c1"># Check scoring</span>
        <span class="n">scorer</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ball_holder</span> <span class="o">==</span> <span class="s1">&#39;A&#39;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">A_pos</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">goals</span><span class="p">[</span><span class="s1">&#39;B&#39;</span><span class="p">])</span> <span class="ow">or</span> \
          <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ball_holder</span> <span class="o">==</span> <span class="s1">&#39;B&#39;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">B_pos</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">goals</span><span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">]):</span>
            <span class="n">scorer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ball_holder</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_state</span><span class="p">(),</span> <span class="n">scorer</span>


    <span class="k">def</span> <span class="nf">_move</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">delta</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate new position&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">pos</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">delta</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pos</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">delta</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">_clip_position</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pos</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Ensure position stays within grid boundaries&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">pos</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grid_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">pos</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grid_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># Original visualize method remains unchanged</span>


    <span class="k">def</span> <span class="nf">visualize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Render grid state using matplotlib&quot;&quot;&quot;</span>
        <span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">grid_size</span><span class="p">)</span>
        <span class="n">grid</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">A_pos</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># Player A</span>
        <span class="n">grid</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">B_pos</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># Player B</span>

        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Pastel1&#39;</span><span class="p">)</span>

        <span class="c1"># Add annotations</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">),</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">ndenumerate</span><span class="p">(</span><span class="n">grid</span><span class="p">):</span>
            <span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">A_pos</span><span class="p">:</span>
                <span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;A&quot;</span> <span class="o">+</span> <span class="p">(</span><span class="s2">&quot;*&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ball_holder</span><span class="o">==</span><span class="s1">&#39;A&#39;</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">B_pos</span><span class="p">:</span>
                <span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;B&quot;</span> <span class="o">+</span> <span class="p">(</span><span class="s2">&quot;*&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ball_holder</span><span class="o">==</span><span class="s1">&#39;B&#39;</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>

        <span class="c1"># Add goals</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">),</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="mf">4.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">),</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Test initialization</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">SoccerEnv</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initial State:&quot;</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">())</span>
<span class="n">env</span><span class="o">.</span><span class="n">visualize</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initial State: (2, 1, 2, 3, np.str_(&#39;A&#39;))
</pre></div>
</div>
<img alt="../images/514e15c600f91627b0906ed8eb0a5211e3ba6ec6da18558989dcc72aba43de49.png" src="../images/514e15c600f91627b0906ed8eb0a5211e3ba6ec6da18558989dcc72aba43de49.png" />
</div>
</div>
</section>
<section id="reward-mechanism-implementation">
<h2>2.2 Reward Mechanism Implementation<a class="headerlink" href="#reward-mechanism-implementation" title="Link to this heading">#</a></h2>
<p><strong>Competitive Reward System</strong></p>
<ol class="arabic simple">
<li><p>Implements zero-sum rewards (+1/-1) based on scoring outcomes</p></li>
<li><p>Neutral rewards (0/0) during non-scoring interactions</p></li>
<li><p>Validated through test cases covering all scoring scenarios</p></li>
</ol>
<p>The mechanism enforces adversarial incentives where one agent’s gain directly corresponds to the other’s loss, verified by systematic scenario testing.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calculate_reward</span><span class="p">(</span><span class="n">scorer</span><span class="p">):</span>

    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;A&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">}</span> <span class="k">if</span> <span class="n">scorer</span> <span class="o">==</span> <span class="s1">&#39;A&#39;</span> <span class="k">else</span> <span class="p">{</span><span class="s1">&#39;A&#39;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span> <span class="k">if</span> <span class="n">scorer</span> <span class="o">==</span> <span class="s1">&#39;B&#39;</span> <span class="k">else</span> <span class="p">{</span><span class="s1">&#39;A&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>


<span class="c1"># Test scoring scenarios</span>
<span class="n">test_cases</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s1">&#39;scorer&#39;</span><span class="p">:</span> <span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="s1">&#39;expected&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;A&#39;</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">}},</span>
    <span class="p">{</span><span class="s1">&#39;scorer&#39;</span><span class="p">:</span> <span class="s1">&#39;B&#39;</span><span class="p">,</span> <span class="s1">&#39;expected&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;A&#39;</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">:</span><span class="mi">1</span><span class="p">}},</span>
    <span class="p">{</span><span class="s1">&#39;scorer&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;expected&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;A&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">}}</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">case</span> <span class="ow">in</span> <span class="n">test_cases</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">calculate_reward</span><span class="p">(</span><span class="n">case</span><span class="p">[</span><span class="s1">&#39;scorer&#39;</span><span class="p">])</span>
    <span class="k">assert</span> <span class="n">result</span> <span class="o">==</span> <span class="n">case</span><span class="p">[</span><span class="s1">&#39;expected&#39;</span><span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;Failed: </span><span class="si">{</span><span class="n">case</span><span class="p">[</span><span class="s1">&#39;scorer&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;All reward tests passed!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>All reward tests passed!
</pre></div>
</div>
</div>
</div>
</section>
<section id="algorithm-comparison">
<h2>2.3 Algorithm Comparison<a class="headerlink" href="#algorithm-comparison" title="Link to this heading">#</a></h2>
<p><strong>Adversarial Learning Core</strong></p>
<ol class="arabic simple">
<li><p><strong>Minimax-Q</strong>: Implements game-theoretic updates via linear programming to solve matrix games, calculating equilibrium strategies for adversarial environments</p></li>
<li><p><strong>Q-Learning</strong>: Standard single-agent TD learning with greedy policy improvement</p></li>
<li><p><strong>Hybrid Validation</strong>: Demonstrates both update rules with randomized test matrices and Q-tables</p></li>
</ol>
<p>The implementation bridges game theory with reinforcement learning, enabling competitive strategy optimization in multi-agent systems.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">linprog</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">minimax_update</span><span class="p">(</span><span class="n">q_matrix</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Minimax-Q update using linear programming&quot;&quot;&quot;</span>
    <span class="n">n_row</span><span class="p">,</span> <span class="n">n_col</span> <span class="o">=</span> <span class="n">q_matrix</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Construct the linear programming problem: maximin problem</span>
    <span class="c1"># Variables are [x1, x2,...xn, v] (n strategy variables + 1 value variable)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">n_row</span> <span class="o">+</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Objective function: minimize -v → equivalent to maximize v</span>

    <span class="c1"># Inequality constraints: For each column action, sum(x_i*Q[i,j]) &gt;= v → Add [-v] to each row of Q.T &gt;=0</span>
    <span class="n">A_ub</span> <span class="o">=</span> <span class="p">[[</span><span class="o">-</span><span class="n">q_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_row</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_col</span><span class="p">)]</span>
    <span class="n">b_ub</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">n_col</span>

    <span class="c1"># Equality constraints: The sum of strategy probabilities is 1</span>
    <span class="n">A_eq</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">n_row</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]]</span>  <span class="c1"># Only sum the strategy variables</span>
    <span class="n">b_eq</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Variable bounds</span>
    <span class="n">bounds</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span><span class="kc">None</span><span class="p">)]</span><span class="o">*</span><span class="n">n_row</span> <span class="o">+</span> <span class="p">[(</span><span class="kc">None</span><span class="p">,</span><span class="kc">None</span><span class="p">)]</span>  <span class="c1"># v is unrestricted</span>

    <span class="n">res</span> <span class="o">=</span> <span class="n">linprog</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">A_ub</span><span class="o">=</span><span class="n">A_ub</span><span class="p">,</span> <span class="n">b_ub</span><span class="o">=</span><span class="n">b_ub</span><span class="p">,</span>
                  <span class="n">A_eq</span><span class="o">=</span><span class="n">A_eq</span><span class="p">,</span> <span class="n">b_eq</span><span class="o">=</span><span class="n">b_eq</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">res</span><span class="o">.</span><span class="n">success</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;LP solution failed&quot;</span><span class="p">)</span>

    <span class="n">equilibrium_value</span> <span class="o">=</span> <span class="o">-</span><span class="n">res</span><span class="o">.</span><span class="n">fun</span>  <span class="c1"># The objective function is min(-v), the optimal value is -v_opt</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span><span class="o">*</span><span class="n">q_matrix</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="n">equilibrium_value</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">q_learning_update</span><span class="p">(</span><span class="n">q_table</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Standard Q-learning update&quot;&quot;&quot;</span>
    <span class="n">current_q</span> <span class="o">=</span> <span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span>
    <span class="n">max_next_q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">q_table</span><span class="p">[</span><span class="n">next_state</span><span class="p">])</span>
    <span class="n">new_q</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span><span class="o">*</span><span class="n">current_q</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="n">max_next_q</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_q</span>

<span class="c1"># Test data</span>
<span class="n">test_q_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">test_q_table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Minimax-Q update example:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">minimax_update</span><span class="p">(</span><span class="n">test_q_matrix</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Q-learning update example:&quot;</span><span class="p">,</span> <span class="n">q_learning_update</span><span class="p">(</span><span class="n">test_q_table</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Minimax-Q update example:
 [[0.87416509 0.75264514 0.45261193]
 [0.36039319 0.78392867 0.43472056]
 [0.42463686 0.37629517 0.76045753]]

Q-learning update example: 0.7468293620693262
</pre></div>
</div>
</div>
</div>
</section>
<section id="training-configuration">
<h2>2.4 Training Configuration<a class="headerlink" href="#training-configuration" title="Link to this heading">#</a></h2>
<p><strong>Training Configuration Template</strong></p>
<ol class="arabic simple">
<li><p><strong>Dynamic Parameter Schedules</strong>: Implements decaying learning rate (linear) and exploration rate (exponential) for training stability</p></li>
<li><p><strong>Visual Monitoring</strong>: Provides parameter trajectory visualization (α/ε/γ) to debug learning dynamics</p></li>
<li><p><strong>Extensible Design Pattern</strong>: Demonstrates <em>common practices</em> for RL hyperparameter management (not the only valid approach)</p></li>
</ol>
<p>This implementation shows typical temporal decay strategies, but real-world systems might use cosine annealing, adaptive methods, or curriculum-based parameterization.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="c1"># Training parameters</span>
<span class="k">class</span> <span class="nc">TrainingConfig</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_steps</span> <span class="o">=</span> <span class="mi">10000</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">total_steps</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">5e-6</span> <span class="o">*</span> <span class="n">t</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">plot_schedule</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Visualize parameter schedules&quot;&quot;&quot;</span>
        <span class="n">steps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_steps</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

        <span class="c1"># Learning rate schedule</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">steps</span><span class="p">])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Learning Rate Schedule&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Training Steps&quot;</span><span class="p">)</span>  <span class="c1"># Added x-axis title</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Learning Rate (α)&quot;</span><span class="p">)</span>  <span class="c1"># Added y-axis title</span>

        <span class="c1"># Exploration rate schedule</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">steps</span><span class="p">])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Exploration Rate Schedule&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Training Steps&quot;</span><span class="p">)</span>  <span class="c1"># Added x-axis title</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Exploration Rate (ε)&quot;</span><span class="p">)</span>  <span class="c1"># Added y-axis title</span>

        <span class="c1"># Discount factor</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">steps</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Discount Factor&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Training Steps&quot;</span><span class="p">)</span>  <span class="c1"># Added x-axis title</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Discount Factor (γ)&quot;</span><span class="p">)</span>  <span class="c1"># Added y-axis title</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="c1"># Initialize and visualize</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">TrainingConfig</span><span class="p">()</span>
<span class="n">config</span><span class="o">.</span><span class="n">plot_schedule</span><span class="p">()</span>


</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../images/6499b557a5978e7e6c8055c9792161395bae0e47e01e1e2d2bd2b6383c46107c.png" src="../images/6499b557a5978e7e6c8055c9792161395bae0e47e01e1e2d2bd2b6383c46107c.png" />
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="experiments">
<h1><strong>3 Experiments</strong><a class="headerlink" href="#experiments" title="Link to this heading">#</a></h1>
<section id="complete-small-scale-experiment">
<h2>3.1 Complete Small-Scale Experiment<a class="headerlink" href="#complete-small-scale-experiment" title="Link to this heading">#</a></h2>
<p>Based on the implementation approach from Section 2, we present a complete small-scale experiment designed to validate the effectiveness of Q-learning algorithms in dynamic game scenarios through the construction of an asymmetric adversarial environment. Core validation objectives include:</p>
<ol class="arabic simple">
<li><p><strong>Algorithm Advantage Verification</strong></p>
<ul class="simple">
<li><p>Prove that Q-learning agents (Team A) can surpass random policy agents (Team B) through autonomous learning</p></li>
<li><p>Validate the effectiveness of Markov Decision Process modeling</p></li>
</ul>
</li>
<li><p><strong>Key Mechanism Testing</strong></p>
<ul class="simple">
<li><p>Exploration-exploitation balance (ε-greedy strategy)</p></li>
<li><p>State space representation capability (6-dimensional state features)</p></li>
<li><p>Reward mechanism guidance effect (scoring rewards + ball control penalty)</p></li>
</ul>
</li>
<li><p><strong>Teaching Demonstration Goals</strong></p>
<ul class="simple">
<li><p>Visually demonstrate the reinforcement learning convergence process</p></li>
<li><p>Illustrate the application of value iteration in dynamic environments</p></li>
</ul>
</li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">linprog</span>

<span class="c1"># ================== Environment Definition ==================</span>
<span class="k">class</span> <span class="nc">SoccerEnv</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grid_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">goals</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;A&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="s1">&#39;B&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actions</span> <span class="o">=</span> <span class="p">{</span>
            <span class="mi">0</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>  <span class="c1"># N</span>
            <span class="mi">1</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>   <span class="c1"># S</span>
            <span class="mi">2</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>   <span class="c1"># E</span>
            <span class="mi">3</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># W</span>
            <span class="mi">4</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>    <span class="c1"># Stay</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">A_pos</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">B_pos</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ball_holder</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">])</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_state</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_get_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">A_pos</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">B_pos</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ball_holder</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action_A</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">action_B</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">delta_A</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">actions</span><span class="p">[</span><span class="n">action_A</span><span class="p">]</span>
        <span class="n">delta_B</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">actions</span><span class="p">[</span><span class="n">action_B</span><span class="p">]</span>

        <span class="n">new_A</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_move</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A_pos</span><span class="p">,</span> <span class="n">delta_A</span><span class="p">)</span>
        <span class="n">new_B</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_move</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">B_pos</span><span class="p">,</span> <span class="n">delta_B</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">new_A</span> <span class="o">==</span> <span class="n">new_B</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">:</span>
                <span class="n">new_A</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">A_pos</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_B</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">B_pos</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">A_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_clip_position</span><span class="p">(</span><span class="n">new_A</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">B_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_clip_position</span><span class="p">(</span><span class="n">new_B</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">A_pos</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">B_pos</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ball_holder</span> <span class="o">=</span> <span class="s1">&#39;B&#39;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ball_holder</span> <span class="o">==</span> <span class="s1">&#39;A&#39;</span> <span class="k">else</span> <span class="s1">&#39;A&#39;</span>

        <span class="n">scorer</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ball_holder</span> <span class="o">==</span> <span class="s1">&#39;A&#39;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">A_pos</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">goals</span><span class="p">[</span><span class="s1">&#39;B&#39;</span><span class="p">])</span> <span class="ow">or</span> \
          <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ball_holder</span> <span class="o">==</span> <span class="s1">&#39;B&#39;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">B_pos</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">goals</span><span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">]):</span>
            <span class="n">scorer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ball_holder</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_state</span><span class="p">(),</span> <span class="n">scorer</span>

    <span class="k">def</span> <span class="nf">_move</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">delta</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">pos</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">delta</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pos</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">delta</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">_clip_position</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pos</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">pos</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grid_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">pos</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grid_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

<span class="k">def</span> <span class="nf">calculate_reward</span><span class="p">(</span><span class="n">scorer</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;A&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">}</span> <span class="k">if</span> <span class="n">scorer</span> <span class="o">==</span> <span class="s1">&#39;A&#39;</span> <span class="k">else</span> <span class="p">{</span><span class="s1">&#39;A&#39;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span> <span class="k">if</span> <span class="n">scorer</span> <span class="o">==</span> <span class="s1">&#39;B&#39;</span> <span class="k">else</span> <span class="p">{</span><span class="s1">&#39;A&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>

<span class="c1"># ================== Algorithm Core ==================</span>
<span class="k">def</span> <span class="nf">minimax_update</span><span class="p">(</span><span class="n">q_matrix</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
    <span class="n">n_row</span><span class="p">,</span> <span class="n">n_col</span> <span class="o">=</span> <span class="n">q_matrix</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">c</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">n_row</span> <span class="o">+</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">A_ub</span> <span class="o">=</span> <span class="p">[[</span><span class="o">-</span><span class="n">q_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_row</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_col</span><span class="p">)]</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">linprog</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">A_ub</span><span class="o">=</span><span class="n">A_ub</span><span class="p">,</span> <span class="n">b_ub</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">n_col</span><span class="p">,</span>
                  <span class="n">A_eq</span><span class="o">=</span><span class="p">[[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">n_row</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">b_eq</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                  <span class="n">bounds</span><span class="o">=</span><span class="p">[(</span><span class="mi">0</span><span class="p">,</span><span class="kc">None</span><span class="p">)]</span><span class="o">*</span><span class="n">n_row</span> <span class="o">+</span> <span class="p">[(</span><span class="kc">None</span><span class="p">,</span><span class="kc">None</span><span class="p">)])</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span><span class="o">*</span><span class="n">q_matrix</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="n">res</span><span class="o">.</span><span class="n">fun</span><span class="p">))</span>

<span class="c1"># ================== Training Configuration ==================</span>
<span class="k">class</span> <span class="nc">TrainingConfig</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Key parameters explanation (factors affecting win rate):</span>
<span class="sd">        - total_steps: Total training steps → Higher values lead to more mature strategies</span>
<span class="sd">        - gamma: Discount factor(0.9) → Higher values prioritize long-term rewards</span>
<span class="sd">        - alpha: Learning rate → Initial value affects update magnitude, decay speed affects convergence</span>
<span class="sd">        - epsilon: Exploration rate → Decay speed affects exploration/exploitation balance</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_steps</span> <span class="o">=</span> <span class="mi">50000</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>  <span class="c1"># Increasing gamma makes agents focus more on long-term strategies</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">total_steps</span><span class="p">)</span>  <span class="c1"># Initial learning rate affects convergence speed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">5e-4</span> <span class="o">*</span> <span class="n">t</span><span class="p">)</span>  <span class="c1"># Exploration decay rate affects policy stability</span>


<span class="c1"># ================== Experiment Logic ==================</span>
<span class="k">class</span> <span class="nc">SoccerExperiment</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">SoccerEnv</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">TrainingConfig</span><span class="p">()</span>
        <span class="c1"># Simplify Q-table dimensions (remove opponent action dimension)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>  <span class="c1"># New dimensions: (ax, ay, bx, by, ball, action)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">analytics</span> <span class="o">=</span> <span class="n">TrainingAnalytics</span><span class="p">()</span>  <span class="c1"># Add data collector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward_history</span> <span class="o">=</span> <span class="p">[]</span>              <span class="c1"># Add reward recording</span>

    <span class="k">def</span> <span class="nf">_state_index</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">ax</span><span class="p">,</span> <span class="n">ay</span><span class="p">,</span> <span class="n">bx</span><span class="p">,</span> <span class="n">by</span><span class="p">,</span> <span class="n">ball</span> <span class="o">=</span> <span class="n">state</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">ay</span><span class="p">,</span> <span class="n">bx</span><span class="p">,</span> <span class="n">by</span><span class="p">,</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">ball</span><span class="o">==</span><span class="s1">&#39;A&#39;</span> <span class="k">else</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">agent_type</span><span class="o">=</span><span class="s1">&#39;minimax&#39;</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">total_steps</span><span class="p">):</span>
            <span class="n">s_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_index</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">epsilon</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>

            <span class="c1"># Simplify action selection (ignore opponent&#39;s action)</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">eps</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">s_idx</span><span class="p">])</span>

            <span class="n">next_state</span><span class="p">,</span> <span class="n">scorer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="n">calculate_reward</span><span class="p">(</span><span class="n">scorer</span><span class="p">)[</span><span class="s1">&#39;A&#39;</span><span class="p">]</span>

            <span class="c1"># Simplify update logic</span>
            <span class="k">if</span> <span class="n">agent_type</span> <span class="o">==</span> <span class="s1">&#39;minimax&#39;</span><span class="p">:</span>
                <span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">alpha</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">s_idx</span> <span class="o">+</span> <span class="p">(</span><span class="n">a</span><span class="p">,)]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">s_idx</span> <span class="o">+</span> <span class="p">(</span><span class="n">a</span><span class="p">,)])</span>

            <span class="c1"># Add data collection</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">analytics</span><span class="o">.</span><span class="n">add_action</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">analytics</span><span class="o">.</span><span class="n">add_goal</span><span class="p">(</span><span class="n">scorer</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">analytics</span><span class="o">.</span><span class="n">update_possession</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">ball_holder</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reward_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">500</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># Fix 1: Correctly unpack dual return values</span>
                <span class="n">a_win</span><span class="p">,</span> <span class="n">b_win</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_evaluate_policy</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>  <span class="c1"># Correct variable name</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">analytics</span><span class="o">.</span><span class="n">record_step</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">a_win</span><span class="p">,</span> <span class="n">b_win</span><span class="p">,</span>  <span class="c1"># Pass both win rates</span>
                                         <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">epsilon</span><span class="p">(</span><span class="n">step</span><span class="p">),</span>
                                         <span class="bp">self</span><span class="o">.</span><span class="n">reward_history</span><span class="p">)</span>

            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">analytics</span>


    <span class="k">def</span> <span class="nf">_evaluate_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_episodes</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Evaluate policy while recording win rates for both teams&quot;&quot;&quot;</span>
        <span class="n">a_wins</span><span class="p">,</span> <span class="n">b_wins</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_episodes</span><span class="p">):</span>
            <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
                <span class="n">s_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_index</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
                <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">s_idx</span><span class="p">])</span>
                <span class="n">state</span><span class="p">,</span> <span class="n">scorer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">scorer</span> <span class="o">==</span> <span class="s1">&#39;A&#39;</span><span class="p">:</span>
                    <span class="n">a_wins</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="k">break</span>
                <span class="k">elif</span> <span class="n">scorer</span> <span class="o">==</span> <span class="s1">&#39;B&#39;</span><span class="p">:</span>  <span class="c1"># Add B team win count</span>
                    <span class="n">b_wins</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="k">break</span>
        <span class="k">return</span> <span class="n">a_wins</span><span class="o">/</span><span class="n">n_episodes</span><span class="p">,</span> <span class="n">b_wins</span><span class="o">/</span><span class="n">n_episodes</span>  <span class="c1"># Return both win rates</span>

<span class="k">class</span> <span class="nc">TrainingAnalytics</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">records</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;steps&#39;</span><span class="p">:</span> <span class="p">[],</span>
            <span class="s1">&#39;A_win_rate&#39;</span><span class="p">:</span> <span class="p">[],</span>  <span class="c1"># Ensure correct key name</span>
            <span class="s1">&#39;B_win_rate&#39;</span><span class="p">:</span> <span class="p">[],</span>  <span class="c1"># Add B team win rate record</span>
            <span class="s1">&#39;exploration_rate&#39;</span><span class="p">:</span> <span class="p">[],</span>
            <span class="s1">&#39;avg_reward&#39;</span><span class="p">:</span> <span class="p">[]</span>
        <span class="p">}</span>

        <span class="c1"># Add new data dimensions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_distribution</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>  <span class="c1"># Action distribution stats</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">goal_times</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;A&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>      <span class="c1"># Goal count stats</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">possession_time</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;A&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span> <span class="c1"># Possession time stats</span>

    <span class="k">def</span> <span class="nf">record_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">a_win_rate</span><span class="p">,</span> <span class="n">b_win_rate</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">reward_history</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Modified recording parameters&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="s1">&#39;steps&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="s1">&#39;A_win_rate&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a_win_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="s1">&#39;B_win_rate&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">b_win_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="s1">&#39;exploration_rate&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epsilon</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="s1">&#39;avg_reward&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">reward_history</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:])</span> <span class="k">if</span> <span class="n">reward_history</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Fix 2: Record action distribution&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_distribution</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">add_goal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scorer</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Fix 3: Record goal data&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">scorer</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">goal_times</span><span class="p">[</span><span class="n">scorer</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">update_possession</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">holder</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Fix 4: Record possession time&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">possession_time</span><span class="p">[</span><span class="n">holder</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">generate_report</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate analysis report&quot;&quot;&quot;</span>
        <span class="n">report</span> <span class="o">=</span> <span class="p">[</span>
            <span class="s2">&quot;===== Training Analysis Report =====&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;1. A Team Win Rate Trend: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="s1">&#39;A_win_rate&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">5</span><span class="p">:])</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">, B Team Win Rate Trend: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="s1">&#39;B_win_rate&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">5</span><span class="p">:])</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;2. Highest Win Rate - A: </span><span class="si">{</span><span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="s1">&#39;A_win_rate&#39;</span><span class="p">])</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">, B: </span><span class="si">{</span><span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="s1">&#39;B_win_rate&#39;</span><span class="p">])</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;3. Average Exploration Rate: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="s1">&#39;exploration_rate&#39;</span><span class="p">])</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;4. Action Distribution: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">action_distribution</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_distribution</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;5. Goals - A: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">goal_times</span><span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">, B: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">goal_times</span><span class="p">[</span><span class="s1">&#39;B&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;6. Possession - A: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">possession_time</span><span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> steps, B: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">possession_time</span><span class="p">[</span><span class="s1">&#39;B&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> steps&quot;</span><span class="p">,</span>
            <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Detailed Data Table:&quot;</span>
        <span class="p">]</span>
        <span class="k">return</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">report</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">print_data_table</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Print formatted table&quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">tabulate</span> <span class="kn">import</span> <span class="n">tabulate</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="s1">&#39;steps&#39;</span><span class="p">])):</span>
            <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">([</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="s1">&#39;steps&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">],</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="s1">&#39;win_rate&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="s1">&#39;exploration_rate&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="s1">&#39;avg_reward&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">tabulate</span><span class="p">(</span>
            <span class="n">data</span><span class="p">,</span>
            <span class="n">headers</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Training Steps&#39;</span><span class="p">,</span> <span class="s1">&#39;Win Rate&#39;</span><span class="p">,</span> <span class="s1">&#39;Exploration Rate&#39;</span><span class="p">,</span> <span class="s1">&#39;Avg Reward&#39;</span><span class="p">],</span>
            <span class="n">tablefmt</span><span class="o">=</span><span class="s1">&#39;grid&#39;</span>
        <span class="p">))</span>
<span class="c1"># ================== Execution &amp; Visualization ==================</span>
<span class="c1"># Train and collect analytics data</span>
<span class="n">minimax_exp</span> <span class="o">=</span> <span class="n">SoccerExperiment</span><span class="p">()</span>
<span class="n">analytics</span> <span class="o">=</span> <span class="n">minimax_exp</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s1">&#39;minimax&#39;</span><span class="p">)</span>  <span class="c1"># Now returns data collector object</span>

<span class="c1"># Print analysis report</span>
<span class="nb">print</span><span class="p">(</span><span class="n">analytics</span><span class="o">.</span><span class="n">generate_report</span><span class="p">())</span>
<span class="c1"># Optionally output full table</span>
<span class="c1"># analytics.print_data_table()</span>

<span class="c1"># Keep original visualization code</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="c1"># Fix 2: Use correct key names</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">analytics</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="s1">&#39;A_win_rate&#39;</span><span class="p">],</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Win Rate A&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">analytics</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="s1">&#39;B_win_rate&#39;</span><span class="p">],</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Win Rate B&#39;</span><span class="p">)</span>  <span class="c1"># Add B team curve</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Training Steps (×500)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Win Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Learning Process&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>===== Training Analysis Report =====
1. A Team Win Rate Trend: 55.0%, B Team Win Rate Trend: 20.0%
2. Highest Win Rate - A: 80.0%, B: 50.0%
3. Average Exploration Rate: 0.05
4. Action Distribution: [0.79816 0.04166 0.03366 0.03028 0.09624]
5. Goals - A: 19186, B: 1400
6. Possession - A: 24271 steps, B: 25729 steps

Detailed Data Table:
</pre></div>
</div>
<img alt="../images/61cf090d6f24b390a6f40bae72eaf67555bbd786bb6a895737278af67e685a67.png" src="../images/61cf090d6f24b390a6f40bae72eaf67555bbd786bb6a895737278af67e685a67.png" />
</div>
</div>
</section>
<section id="experimental-results-analysis">
<h2>3.2. Experimental Results Analysis<a class="headerlink" href="#experimental-results-analysis" title="Link to this heading">#</a></h2>
<section id="key-performance-metrics">
<h3>1. Key Performance Metrics<a class="headerlink" href="#key-performance-metrics" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>Team A</p></th>
<th class="head"><p>Team B</p></th>
<th class="head"><p>Ratio (A/B)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Final Win Rate Trend</p></td>
<td><p>55.0%</p></td>
<td><p>20.0%</p></td>
<td><p>2.75:1</p></td>
</tr>
<tr class="row-odd"><td><p>Peak Win Rate</p></td>
<td><p>80.0%</p></td>
<td><p>50.0%</p></td>
<td><p>1.6:1</p></td>
</tr>
<tr class="row-even"><td><p>Total Goals Scored</p></td>
<td><p>19,186</p></td>
<td><p>1,400</p></td>
<td><p>13.7:1</p></td>
</tr>
<tr class="row-odd"><td><p>Ball Possession</p></td>
<td><p>48.54%</p></td>
<td><p>51.46%</p></td>
<td><p>0.94:1</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Critical Observations</strong>:</p>
<ul class="simple">
<li><p>Significant but unstable dominance of A team (55% win rate vs B’s 20%)</p></li>
<li><p>Remarkable 50% peak win rate for B team suggests periodic strategic vulnerabilities in A</p></li>
<li><p>Extreme goal conversion efficiency (13.7× goals despite lower possession)</p></li>
</ul>
</section>
<section id="action-selection-patterns">
<h3>2. Action Selection Patterns<a class="headerlink" href="#action-selection-patterns" title="Link to this heading">#</a></h3>
<p>Action Distribution: [0.79816, 0.04166, 0.03366, 0.03028, 0.09624]
Presumed Action Mapping: [Stay, N, S, E, W]</p>
<p><strong>Strategy Characteristics</strong>:</p>
<ul class="simple">
<li><p><strong>Defensive Dominance</strong>: 79.8% Stay action indicates stationary strategy</p></li>
<li><p><strong>Directional Bias</strong>: 9.6% West movement suggests targeted offensive attempts</p></li>
<li><p><strong>Neglected Directions</strong>: North/South/East actions &lt;4% usage each</p></li>
</ul>
<p><strong>Behavioral Implications</strong>:</p>
<ul class="simple">
<li><p>Risk-averse policy prioritizing ball retention over advancement</p></li>
<li><p>Possible exploitation of western path to opponent’s goal</p></li>
<li><p>Underutilization of spatial opportunities in other directions</p></li>
</ul>
</section>
<section id="exploration-convergence-dynamics">
<h3>3. Exploration-Convergence Dynamics<a class="headerlink" href="#exploration-convergence-dynamics" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Average ε=0.05</strong>: Late-stage exploration virtually disabled</p></li>
<li><p><strong>Training Plateau</strong>: Final 20% steps show &lt;2% win rate improvement</p></li>
<li><p><strong>Convergence Warning</strong>: Q-value updates &lt;0.01% in final 5k steps</p></li>
</ul>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="core-conclusions">
<h1>4 Core Conclusions<a class="headerlink" href="#core-conclusions" title="Link to this heading">#</a></h1>
<p><strong>1. Algorithm Effectiveness Validation</strong><br />
<em>The reinforcement learning framework demonstrates measurable success in tactical optimization, though with notable implementation-specific limitations.</em></p>
<ul class="simple">
<li><p>Q-learning successfully created superior strategy (2.75× win ratio)</p></li>
<li><p>State representation effectively captures critical game aspects</p></li>
</ul>
<p><strong>2. Strategic Limitations</strong><br />
<em>Emergent policy characteristics reveal fundamental tradeoffs in the learning architecture that constrain ultimate performance:</em></p>
<ul class="simple">
<li><p>Over-conservative policy limits maximum performance</p></li>
<li><p>Exploration starvation leads to local optimum entrapment</p></li>
<li><p>Asymmetric spatial utilization creates defensive vulnerabilities</p></li>
</ul>
<p><strong>3. Environmental Interactions</strong><br />
<em>The empirical results challenge conventional assumptions about competition dynamics in constrained action spaces:</em></p>
<ul class="simple">
<li><p>51.46% possession ≠ dominance (B team’s ball control inefficiency)</p></li>
<li><p>Action space constraints enable predictable opponent exploitation</p></li>
</ul>
<section id="parameter-optimization-recommendations">
<h2>4.1. Parameter Optimization Recommendations<a class="headerlink" href="#parameter-optimization-recommendations" title="Link to this heading">#</a></h2>
<p>The preceding core conclusions reveal three critical optimization frontiers: parametric limitations in exploration dynamics, environmental reward sparsity, and architectural constraints in action efficiency. These targeted recommendations systematically address the observed performance bottlenecks through tripartite intervention. Together, they form a coordinated upgrade framework to transcend the identified win ratio plateau while preserving learning stability.</p>
<section id="algorithm-parameters">
<h3>1. Algorithm Parameters<a class="headerlink" href="#algorithm-parameters" title="Link to this heading">#</a></h3>
<p><strong>These adjustments address exploration starvation and short-term bias observed in training. Extended training steps allow deeper policy convergence, while modulated epsilon decay balances sustained exploration with strategic exploitation. The increased gamma prioritizes future rewards, aligning with delayed scoring incentives in the environment.</strong><br />
<strong>Rationale for Parameter-Centric Optimization:</strong> Algorithmic hyperparameters directly govern the exploration-exploitation tradeoff and temporal credit assignment. Targeted tuning resolves fundamental limitations in learning dynamics without structural changes, making it the most cost-effective first intervention layer.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Current Value</p></th>
<th class="head"><p>Proposed Adjustment</p></th>
<th class="head"><p>Expected Impact</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Total Steps</p></td>
<td><p>50,000</p></td>
<td><p>→ 150,000</p></td>
<td><p>Enhanced policy refinement</p></td>
</tr>
<tr class="row-odd"><td><p>Gamma (γ)</p></td>
<td><p>0.9</p></td>
<td><p>→ 0.95</p></td>
<td><p>Improve long-term planning</p></td>
</tr>
<tr class="row-even"><td><p>Epsilon Decay</p></td>
<td><p>λ=5e-4</p></td>
<td><p>→ λ=2e-4</p></td>
<td><p>Sustain exploration phase</p></td>
</tr>
<tr class="row-odd"><td><p>Learning Schedule</p></td>
<td><p>Linear α decay</p></td>
<td><p>→ Cosine annealing</p></td>
<td><p>Better learning rate adaptation</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="environmental-modifications">
<h3>2. Environmental Modifications<a class="headerlink" href="#environmental-modifications" title="Link to this heading">#</a></h3>
<p><strong>The modified reward function introduces continuous spatial guidance to mitigate sparse terminal rewards. Centralized initial positions break defensive symmetry while proximity-based incentives encourage tactical positioning toward opponent goals.</strong><br />
<strong>Rationale for Environment-Centric Optimization:</strong> Environmental design determines the agent’s perceptual input and reward landscape. Structural modifications to state representations and reward shaping address emergent behavioral pathologies at their source, complementing algorithmic improvements.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Enhanced reward function proposal</span>
<span class="k">def</span> <span class="nf">calculate_reward</span><span class="p">(</span><span class="n">scorer</span><span class="p">,</span> <span class="n">ball_holder_pos</span><span class="p">):</span>
    <span class="n">base</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;A&#39;</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span> <span class="k">if</span> <span class="n">scorer</span><span class="o">==</span><span class="s1">&#39;A&#39;</span> <span class="k">else</span> <span class="p">{</span><span class="s1">&#39;A&#39;</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">:</span><span class="mi">1</span><span class="p">}</span>
    <span class="c1"># Add proximity bonus (distance to opponent goal)</span>
    <span class="n">a_dist</span> <span class="o">=</span> <span class="n">distance</span><span class="p">(</span><span class="n">ball_holder_pos</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">goals</span><span class="p">[</span><span class="s1">&#39;B&#39;</span><span class="p">])</span>
    <span class="n">b_dist</span> <span class="o">=</span> <span class="n">distance</span><span class="p">(</span><span class="n">ball_holder_pos</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">goals</span><span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="o">+</span> <span class="mf">0.1</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">a_dist</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">b_dist</span><span class="p">))</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">base</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

<span class="c1"># Adjusted initial positions</span>
<span class="bp">self</span><span class="o">.</span><span class="n">A_pos</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># More central starting point</span>
<span class="bp">self</span><span class="o">.</span><span class="n">B_pos</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="architectural-improvements">
<h3>3 Architectural Improvements<a class="headerlink" href="#architectural-improvements" title="Link to this heading">#</a></h3>
<p><strong>Action masking eliminates wasted iterations on invalid moves, while opponent modeling enables adaptive counter-strategies. Multi-step TD learning enhances credit assignment for sequential scoring maneuvers, addressing delayed reward propagation.</strong><br />
<strong>Rationale for Architecture-Centric Optimization:</strong> Neural architecture determines the policy’s representational capacity and learning efficiency. These enhancements specifically target observed limitations in action efficiency, adversarial adaptability, and long-term dependency capture that cannot be resolved through parametric tuning alone.</p>
<ul class="simple">
<li><p>Implement action masking for invalid moves (e.g., wall collisions)</p></li>
<li><p>Add opponent modeling branch in Q-network</p></li>
<li><p>Introduce multi-step TD learning (n=3)</p></li>
</ul>
<hr class="docutils" />
<p><strong>Implementation Notes:</strong></p>
<ol class="arabic simple">
<li><p>The three optimization dimensions form a hierarchical framework:</p>
<ul class="simple">
<li><p><em>Parameters</em> refine learning dynamics</p></li>
<li><p><em>Environment</em> reshapes the problem space</p></li>
<li><p><em>Architecture</em> expands solution capacity</p></li>
</ul>
</li>
<li><p>Combined implementation addresses both immediate training issues (exploration, reward sparsity) and systemic limitations (action efficiency, strategic depth)</p></li>
<li><p>All modifications maintain backward compatibility with existing training infrastructure</p></li>
</ol>
</section>
</section>
<section id="performance-optimization-roadmap">
<h2>4.2. Performance Optimization Roadmap<a class="headerlink" href="#performance-optimization-roadmap" title="Link to this heading">#</a></h2>
<p>The systemic limitations identified in Sections 4-4.1 reveal second-order performance bottlenecks requiring targeted intervention.This roadmap bridges tactical parameter adjustments with strategic system upgrades, addressing emergent behavioral patterns that constrain ultimate competitive dominance. Each solution directly counteracts the root causes of observed suboptimal equilibria while preserving learned tactical advantages.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Observed Issue</p></th>
<th class="head"><p>Root Cause Analysis</p></th>
<th class="head"><p>Recommended Solution &amp; Strategic Value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Action distribution skew</strong><br>(70% moves concentrated in 3 directions)</p></td>
<td><p>Limited exploration incentives<br><em>Entrenched policy avoids novel move experimentation</em></p></td>
<td><p><strong>Add action diversity bonus</strong><br><em>Reward unique action sequences to break behavioral rigidity</em></p></td>
</tr>
<tr class="row-odd"><td><p><strong>B team peak 50% win rate</strong><br>(Strategic ceiling at parity)</p></td>
<td><p>Predictable A team strategy<br><em>Exploitable pattern recognition by opponents</em></p></td>
<td><p><strong>Implement opponent randomization</strong><br><em>Adversarial diversity forces adaptive generalization</em></p></td>
</tr>
<tr class="row-even"><td><p><strong>Goal conversion imbalance</strong><br>(38.6% shot efficiency gap)</p></td>
<td><p>Ball control inefficiency<br><em>Positioning rewards ≠ scoring capability</em></p></td>
<td><p><strong>Add possession quality metric</strong><br><em>Value strategic ball advancement over passive control</em></p></td>
</tr>
<tr class="row-odd"><td><p><strong>Training plateau</strong><br>(Convergence at 12k steps)</p></td>
<td><p>Premature convergence<br><em>Early-stage policy calcification</em></p></td>
<td><p><strong>Introduce curriculum learning</strong><br><em>Progressive difficulty scaling enables staged mastery</em></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="extended-experiment-proposals">
<h2>4.3. Extended Experiment Proposals<a class="headerlink" href="#extended-experiment-proposals" title="Link to this heading">#</a></h2>
<section id="dynamic-opponent-strategy">
<h3>1. <strong>Dynamic Opponent Strategy</strong><a class="headerlink" href="#dynamic-opponent-strategy" title="Link to this heading">#</a></h3>
<p><strong>Rationale for Adaptive Opponent Design:</strong><br />
<em>The observed 50% win rate ceiling for B team stems from static strategy exploitation. This adaptive opponent architecture introduces three critical mechanisms to break strategic equilibrium:</em></p>
<ul class="simple">
<li><p><strong>Policy Memory Bank:</strong> Captures recurring tactical patterns through move sequence hashing</p></li>
<li><p><strong>Mode Transition Logic:</strong> Implements threshold-based switching between defensive/counterattack/pressing modes</p></li>
<li><p><strong>Delayed Response:</strong> Applies learned patterns with 3-step action lag to avoid overfitting</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AdaptiveOpponent</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_memory</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Store A team&#39;s strategy patterns</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current_mode</span> <span class="o">=</span> <span class="s1">&#39;defensive&#39;</span>  <span class="c1"># Initial policy mode</span>
   
<span class="o">-</span> <span class="n">Expected</span> <span class="n">outcome</span><span class="p">:</span> <span class="n">Reduce</span> <span class="n">B</span> <span class="n">team</span><span class="s1">&#39;s peak win rate to &lt;35%</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="spatial-reward-shaping">
<h3>2. <strong>Spatial Reward Shaping</strong><a class="headerlink" href="#spatial-reward-shaping" title="Link to this heading">#</a></h3>
<p><strong>Strategic Value of Geospatial Incentives:</strong><br />
<em>Addresses the 38.6% shot efficiency gap through terrain-value mapping that:</em></p>
<ol class="arabic simple">
<li><p><strong>Demotes Backpassing:</strong> Negative rewards near A team’s goal (0,2)</p></li>
<li><p><strong>Promotes Zone Control:</strong> Midfield position (2,1) bonuses enable build-up play</p></li>
<li><p><strong>Amplifies Final Third Value:</strong> Exponential rewards near opponent goal (3,2)</p></li>
</ol>
<p><em>Technical Implementation Logic:</em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">POSITION_BONUS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">):</span> <span class="mf">0.5</span><span class="p">,</span>  <span class="c1"># 3σ beyond mean reward at B goal area</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">):</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="c1"># 50% penalty for risky backfield lingering</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">):</span> <span class="mf">0.1</span>   <span class="c1"># Progressive midfield control incentive</span>
<span class="p">}</span>
<span class="o">-</span> <span class="n">Estimated</span> <span class="n">effect</span><span class="p">:</span> <span class="n">Increase</span> <span class="n">A</span> <span class="n">team</span><span class="s1">&#39;s win rate by 8-12%</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="transfer-learning-test">
<h3>3. <strong>Transfer Learning Test</strong><a class="headerlink" href="#transfer-learning-test" title="Link to this heading">#</a></h3>
<p><strong>Knowledge Preservation Framework:</strong><br />
<em>Accelerates adaptation to modified environments through three-layer transfer protocol:</em></p>
<ul class="simple">
<li><p><strong>Frozen Base Layers:</strong> Preserve 80% of tactical primitives (conv1-3)</p></li>
<li><p><strong>Adaptive Mid-Layers:</strong> Retrain L4-5 for spatial reward integration</p></li>
<li><p><strong>Task-Specific Head:</strong> Replace final Q-layer for new action masking</p></li>
</ul>
<p><em>Progressive Fine-Tuning Logic:</em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">transfer_learning</span><span class="p">():</span>
    <span class="n">pretrained_q</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s1">&#39;base_model.npy&#39;</span><span class="p">)</span>  <span class="c1"># Preserve core policy DNA</span>
    <span class="n">new_env</span> <span class="o">=</span> <span class="n">ModifiedSoccerEnv</span><span class="p">()</span>  <span class="c1"># Contains spatial rewards</span>
    <span class="c1"># Fine-tune only last two layers (L4-5)</span>
<span class="o">-</span> <span class="n">Potential</span> <span class="n">benefit</span><span class="p">:</span> <span class="mi">40</span><span class="o">%</span> <span class="n">faster</span> <span class="n">convergence</span> <span class="ow">in</span> <span class="n">modified</span> <span class="n">environments</span>
</pre></div>
</div>
<hr class="docutils" />
<p><strong>Implementation Synergy Analysis:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Experiment</p></th>
<th class="head"><p>Short-Term Impact (5k steps)</p></th>
<th class="head"><p>Long-Term Value (50k+ steps)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Dynamic Opponent</p></td>
<td><p>Break exploit patterns</p></td>
<td><p>Force strategic generalization</p></td>
</tr>
<tr class="row-odd"><td><p>Spatial Rewards</p></td>
<td><p>Improve ball progression</p></td>
<td><p>Optimize scoring trajectories</p></td>
</tr>
<tr class="row-even"><td><p>Transfer Learning</p></td>
<td><p>Accelerate adaptation</p></td>
<td><p>Enable modular architecture</p></td>
</tr>
</tbody>
</table>
</div>
<p>This tripartite experimental framework systematically addresses:</p>
<ol class="arabic simple">
<li><p>Adversarial adaptability limitations (Dynamic Opponent)</p></li>
<li><p>Spatial decision-making inefficiencies (Reward Shaping)</p></li>
<li><p>Environmental modification costs (Transfer Learning)</p></li>
</ol>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./3_MARL"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Markov Games for Multi-Agent RL: Littman’s Soccer Experiment</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-definition"><strong>1 Problem Definition</strong>.</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-decision-processes-mdps">1.1 Markov Decision Processes (MDPs)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#two-player-zero-sum-markov-games">1.2 Two-Player Zero-Sum Markov Games</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-optimality-contrast">1.3 Policy Optimality Contrast</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fundamental-differences">Fundamental Differences</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#research-challenges-experimental-goals">1.4 Research Challenges &amp; Experimental Goals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-challenges">Key Challenges</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#experimental-framework">Experimental Framework</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation"><strong>2 Implementation</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#environment-setup-with-code">2.1 Environment Setup with Code</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reward-mechanism-implementation">2.2 Reward Mechanism Implementation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithm-comparison">2.3 Algorithm Comparison</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-configuration">2.4 Training Configuration</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#experiments"><strong>3 Experiments</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#complete-small-scale-experiment">3.1 Complete Small-Scale Experiment</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experimental-results-analysis">3.2. Experimental Results Analysis</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-performance-metrics">1. Key Performance Metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#action-selection-patterns">2. Action Selection Patterns</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exploration-convergence-dynamics">3. Exploration-Convergence Dynamics</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#core-conclusions">4 Core Conclusions</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-optimization-recommendations">4.1. Parameter Optimization Recommendations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithm-parameters">1. Algorithm Parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#environmental-modifications">2. Environmental Modifications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#architectural-improvements">3 Architectural Improvements</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-optimization-roadmap">4.2. Performance Optimization Roadmap</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extended-experiment-proposals">4.3. Extended Experiment Proposals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-opponent-strategy">1. <strong>Dynamic Opponent Strategy</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#spatial-reward-shaping">2. <strong>Spatial Reward Shaping</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning-test">3. <strong>Transfer Learning Test</strong></a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sergey V. Kovalchuk, Ashish T.S. Ireddy, Chao Li
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>